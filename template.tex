\documentclass[pdftex,english,oribibl]{llncs}

%% Spracheinstellungen laden
\usepackage[english]{babel}

%% Schriftart in der Ausgabe/Eingabe
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{subfigure}
\usepackage{subcaption}
%% Zitate
\usepackage[numbers]{natbib}
\bibliographystyle{abbrvnat}
%\bibliographystyle{dinat}
%\bibliographystyle{plainnat}
%\bibliographystyle{splncs}
%% Similar to option "sectionbib" but \refname instead of \bibname
\makeatletter
\renewcommand\bibsection{\section*{\refname\@mkboth{\MakeUppercase{\refname}}{\MakeUppercase{\refname}}}}
\makeatother

%% Index
%\usepackage{makeidx}
%\makeindex

%% PDF Einstellungen
% muss nach natbib geladen werden!
\usepackage{nameref}
\usepackage{varioref}
\usepackage[pdfusetitle,pdftex,colorlinks]{hyperref}
\hypersetup{pdfborder={0 0 0}}
\hypersetup{bookmarksdepth=3}
\hypersetup{bookmarksopen=true}
\hypersetup{bookmarksopenlevel=1}
\hypersetup{bookmarksnumbered=true}
\usepackage{color}
\hypersetup{colorlinks=false}

%\usepackage[section]{tocbibind}

\makeatletter
\gdef\@keywords{}
\def\keywords#1{\gdef\@keywords{#1}}
\gdef\@subtitle{}
\def\subtitle#1{\gdef\@subtitle{#1}}

%% modified from llncs
\renewenvironment{abstract}{%
  \list{}{\advance\topsep by0.35cm\relax\small%
          \leftmargin=1cm%
          \labelwidth=\z@%
          \listparindent=\z@%
          \itemindent\listparindent%
          \rightmargin\leftmargin}%
          \item[\hskip\labelsep\bfseries\abstractname]}{%
  \if!\@keywords!\else{\item[~]\item[\hskip\labelsep\bfseries\keywordname]\@keywords}\fi%
  \endlist}

\AtBeginDocument{%
  \if!\@subtitle!\else\hypersetup{pdfsubject={\@subtitle}}\fi
  \if!\@keywords!\else\hypersetup{pdfkeywords={\@keywords}}\fi
}
\makeatother

% llncs hyperref fix
\makeatletter
\providecommand*{\toclevel@author}{0}
\providecommand*{\toclevel@title}{0}
\makeatother

%% Grafiken
\usepackage[pdftex]{graphicx}
\DeclareGraphicsExtensions{.pdf,.jpg,.png}
\usepackage{subfigure}

%% Mathe
\usepackage{amsmath}
\usepackage{amssymb}

%% Listings
\usepackage{listings}
\lstset{escapechar=\%, frame=tb, basicstyle=\footnotesize}

%% Sonstiges
\newcommand{\TODO}[1]{\par\textcolor{red}{#1}\marginpar{\textcolor{red}{TODO}}}
\newcommand{\TODOX}[1]{\textcolor{red}{#1}\marginpar{\textcolor{red}{TODO}}}
\pagestyle{plain}

% Keine "Schusterjungen"
\clubpenalty = 10000
% Keine "Hurenkinder"
\widowpenalty = 10000 \displaywidowpenalty = 10000

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% BEGIN DOCUMENT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Evaluation of Data Quality Assessment}
% \subtitle{My (optional) Subtitle}
\author{KUANG-YU LI}
\institute{University of Stuttgart\\Master Student in Information Technology \\70569 Stuttgart, Germany}


\begin{document}

\maketitle

\begin{abstract}
The paper provides an evaluation of data quality (DQ) assessment methods. The research goals of this paper are to investigate the overview of the DQ method, properties of data, and DQ improvement methods, and application of assessment methods. Due to the diversity and complexity of the topic, fundamentals of data quality and its problem are introduced first. The overview of assessment methods including phases, strategies, techniques, cost, and classification are then presented. In the third section, a case study on Total Data Quality Management (TDQM) is explained with detail, illustrative example, and related applications. In the last section, a conclusion is drawn and future challenges for the DQ assessment method in the big data era are pointed out.

\end{abstract}
\section{Introduction}
With Machine Learning, Deep Learning, and Artificial Intelligence on the rise, the demand for data analysis has never been higher.
Access to a research data set is an important step, but researchers should also be aware of its quality and potential caveats. Traditionally, data labeling and assessment are done in either primitive or laborious manner, which becomes almost impossible in the big data era.
This seminar paper discussed key aspects of data quality systematically.
The goal of this paper is to survey on the following research topics for evaluation of assessing data quality:
\begin{enumerate}
    \item An overview of data quality assessment methods
    \item Data properties that can be analyzed and cannot be analyzed
    \item Possible methods for improving the quality of the data
    \item The current application of the assessment methods to the data set.
\end{enumerate}

The paper is structured as follows: Data Quality Fundamentals, Assessment Methodology Overview,  Case Study, and Conclusion.
To understand the first research topic, data quality assessment methods, the paper first introduced the fundamentals of data quality in the first section. These fundamentals include Data Type, Data Quality Problem, and Data Quality Dimension. With these fundamentals, the reader can have a better understanding of later topics. The overview of the data quality assessment method is introduced in the second section. The pipelines, phases, steps, strategies, techniques, cost, and classification of data quality assessment methods are presented. To address the third and fourth topic,
a method, called Total Data Quality Management (TDQM) are discussed as a case study of the implementation of the DQ improvement.\cite{Wang1998TDQM} The presentation of the method yields a clear picture to the reader. Also in the case study section,  an example on the investment bank and three TDQM real-world applications on the existing data set are presented in subsections of the third section. This response to the research topic of the application of the DQ assessment methods.
At the end of the paper, a conclusion is made based on the evaluation of data quality assessment.  The challenges are discussed in brief. This paper is requested and written under the subtopic of the seminar, Advanced Software Engineering: Non-Functional Aspects in Software Engineering.

For the survey of this seminar, I followed the provided guidelines from "Scientific Working." I first read the literature given by my supervisor.
The recommended literature was from  \citet{Pipino2002DataQualityAssessment, Batini2009MethodologiesForDataQuality, Cai2005ChallnegesOfDataQuality}. After a preliminary review of the literature, I started to gain insight into the topic. Based on the references in the recommended literature, I searched them mainly on Google Scholar, IEEE Explore, and ACM Digital Library.
After reading the extra literature, I picked the useful ones that are central to the research topic and started to structure the seminar paper.  After receiving the peer review from my first submission, I carefully reviewed the feedbacks and revised the paper accordingly.

\section{Data Quality Fundamentals}
In order to understand the data quality assessment method, the fundamentals of data quality are required. The fundamentals include three main subtopics: Data Types, Data Quality Problem, Data Quality Dimensions.  In this section, concepts, definition, and examples are
introduced.

\subsection{Data Types}\label{sec:DataTypes}

The ultimate goal of a DQ methodology is the analysis of data.
In the real world, objects need to be created in a format that could further be stored, retrieved, and processed by software programs.
In the field of data and computer science, data are either implicitly or explicitly distinguished by three types \cite{Batini2009MethodologiesForDataQuality}:

- Structured data, is aggregations or generalizations of items described by elementary attributes defined within a domain.
Domains represent the range of values that can be assigned to attributes and usually correspond to elementary data types of programming languages, such as numeric values or text strings.
Relational tables and statistical data represent the most common type of structured data.

- Unstructured data, is a generic sequence of symbols, typically coded in natural language.
Typical examples of unstructured data are a questionnaire containing free text answering open questions or the body of an e-mail.

- Semistructured data, is data that have a structure which has some degree of flexibility. Semistructured data are also referred to as schemaless or self-describing. For example, XML is markup language commonly used to represent semistructured data.
\begin{comment}
Some common characteristics are:
(1) data can contain fields not known at design time; for instance, an XML file does not have an associated XML schema file;
(2) the same kind of data may be represented in multiple ways; for example, a date might be represented by one field or by multiple fields, even within a single data set; and
(3) among the fields known at design time, many fields will not have values.
\end{comment}

  \begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Paper/figures/DataType.png}
    \caption{Data Type \cite{Batini2009MethodologiesForDataQuality}}
    \label{fig:datatype}
  \end{figure}

 Data quality techniques become increasingly complex as data lose structure. For example, let us consider a registry describing personal information such as Name, Surname, Region, and StateOfBirth. Figure \ref{fig:datatype} shows the representation of Mr. Patrick Metzisi, born in the Masai Mara region in Kenya, by using a structured (Figure \ref{fig:datatype}(a)), unstructured (Figure \ref{fig:datatype}(b)), and semistructured (Figure \ref{fig:datatype}(c)) type of data.
 The same quality dimension will have different metrics according to the type of data. This would be explained in later subsection.
 The large majority of research contributions in the data quality literature focuses on structured and semistructured data. For this reason, this report focuses on structured and semistructured data.

\subsection{Data Quality Problems}\label{sec:DataQualityProblems}
The data quality problem could be classified into four categories depending on two factor: context-independence and role perspective \cite{Borek2011AClassficationOfDataQualityAssessmentMethod}.
 Table \ref{table:DataQualityProblem} provides a brief definition for each DQ problem.
 In the context independent category, spelling errors, missing data, and incorrect values are self-explanatory DQ problems.
 Duplicate data problems occur when rows are duplicated or when schemas contain redundancies.
 Data format problems occur when two or more semantically equivalent data values have different representations, including inconsistent and text formatting.
 Syntax violation problems occur when a pre-specified format has been assigned to an attribute and a data value for this attribute does not adhere to this format, including incomplete data format.
 Problems with violations of integrity constraints arise when data values do not adhere to pre-specified database integrity constraints; we also therefore include unique value violations, rather than have these as a separate problem, because unique value violations are one type of database integrity constraint.
 Note that, despite its position in Table we treat outdated data to be a user perspective problem because whether data is out of date depends on the purpose it is used for.

 For the context dependent category, the problem of violation of domain constraints is when an attribute value must be in a pre-specified context-dependent domain of values.
 Violation of organizational business rules is when any set of values do not adhere to a pre-specified rules assigned by the organization.
 Violation of company and governmental regulations is when any set of values do not adhere to a prespecified rules assigned imposed on the organization by legislating bodies.
 Similarly, violation of constraints provided by the database administrator is when any set of values do not adhere to a pre-specified rules assigned by the database administrator. In section \ref{sec:DataQualityDimensions}, the problems are define in different dimensions with specific indicators.

 % Please add the following required packages to your document preamble:
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
\begin{table}[]
\begin{tabular}{l|l|l|}
\cline{2-3}
                                                                                     & Data Perspective                                                                                                                                                                                                                                                             & User Perspective                                                                                                                                                                                                                                                                                                                                                                \\ \hline
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Context-\\ independent\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Spelling error\\ Missing data\\ Duplicate data\\ Incorrect value\\ Inconsistent data format Outdated data\\ Incomplete data format\\ Syntax violation\\ Unique value violation Violation of \\ integrity constraints Text formatting\end{tabular} & \begin{tabular}[c]{@{}l@{}}The information is inaccessible\\ The information is insecure\\ The information is hardly retrievable \\ The information is difficult to aggregate\\ Errors in the information transformation\end{tabular}                                                                                                                                           \\ \hline
\multicolumn{1}{|l|}{\begin{tabular}[c]{@{}l@{}}Context-\\ dependent\end{tabular}}   & \begin{tabular}[c]{@{}l@{}}Violation of domain constraints \\ Violation of organization's business rules\\ Violation of company and government\\ regulations \\ Violation of constraints provided by\\  the database administrator\end{tabular}                              & \begin{tabular}[c]{@{}l@{}}The information is not based on fact \\ The information is of doubtful credibility\\ The information presents an impartial view\\ The information is irrelevant to the work\\ The information is incomplete\\ The information is compactly represented\\ The information is hard to manipulate \\ The information is hard to understand\end{tabular} \\ \hline
\end{tabular}
\caption{Types of Data Quality Problem \cite{Borek2011AClassficationOfDataQualityAssessmentMethod}}
\label{table:DataQualityProblem}
\end{table}


\subsection{Data Quality Dimensions}\label{sec:DataQualityDimensions}

To define data quality dimension, a data quality is proposed, as shown in Figure \ref{fig:twolayerstandard}. With each dimension, there is underlying data qulity elements and indicators.
\citet{Cai2005ChallnegesOfDataQuality} choose data quality dimensions commonly accepted and widely used as big data quality standards and redefined their basic concepts based on actual business needs.
Each dimension was divided into many typical elements associated with it, and each element has its corresponding quality indicators: Accessibility
, Timeliness
, Authorization
, Credibility
, Definition/Documentation
, MetaData
, Accuracy
, Consistency
, Integrity
, Completeness
, Auditability
, Fitness
, Readability
, and Structure.
In this way, hierarchical quality standards for big data were used for evaluation.
Figure \ref{fig:twolayerstandard} shows a universal two-layer data quality standard. Detailed correspondence dimension, element and indicators are shown in Figure \ref{fig:hierachicalframwork}.

  \begin{figure}
    \centering
    \includegraphics[width=0.25\textwidth]{Paper/figures/DataQualityFramework.png}
    \caption{Data quality framework. \cite{Cai2005ChallnegesOfDataQuality}}
    \label{fig:dataqualityframework}
  \end{figure}

  \begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Paper/figures/TwoLayerStandard.png}
    \caption{A universal, two-layer big data quality standard for assessment. \cite{Cai2005ChallnegesOfDataQuality}}
    \label{fig:twolayerstandard}
  \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Paper/figures/HierarchicalFrameWork.png}
    \caption{The hierarchical big data quality assessment framework. \cite{Cai2005ChallnegesOfDataQuality}}
    \label{fig:hierachicalframwork}
 \end{figure}
Under the same dimension and elements, the indicators varies with type of data structure.
 For instance, syntactic accuracy is measured as described in Section \ref{sec:DataTypes} in the case of structured data. With semistructured data, the distance function should consider a global distance related to the shape of the XML tree in addition to the local distance of fields.

\section{Assessment Methodology Overview}
DQ assessment methodologies can be analyzed and compared through several perspectives.  In this section, the paper introduces the following perspectives:
\begin{itemize}
    \item Phases and Steps that compose the methodology
    \item Strategies and Techniques that are adopted in the methodology for assessing and  improving data quality levels;
    \item Types of Costs, which are associated with data quality issues
\end{itemize}

Methodologies differ in how they consider all of these perspectives. Based on these perspectives, the classification of the DQ method proposed by \citet{Batini2009MethodologiesForDataQuality} is presented.

\subsection{Phases and Steps}

In the most general case, the sequence of activities of a data quality methodology is composed of three phases \cite{Batini2009MethodologiesForDataQuality}.

\begin{enumerate}
    \item \textbf{State Reconstruction}, which is aimed at collecting contextual information on organizational processes and services, data collections and related management procedures, quality issues and corresponding costs; this phase can be skipped if contextual information is available from previous analyses.
    \item \textbf{Assessment}, which measures the quality of data collections along relevant quality dimensions; the term measurement is used to address the issue of measuring the value of a set of data quality dimensions.
    \item \textbf{Improvement}, which concerns the selection of the steps, strategies, and techniques for reaching new data quality targets.
\end{enumerate}

For example, a DQ assessment method called, Comprehensive Data Quality management (CDQ),  employed three of the phases in its method, as shown in Figure \ref{fig:PhasesCDQ}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Paper/figures/PhasesCDQ-2.png}
    \caption{CDQ Phases \cite{Batini2008ComprehensiveDQ}}
    \label{fig:PhasesCDQ}
 \end{figure}

\begin{comment}
The term assessment is used when such measurements are compared to reference values, in order to enable a diagnosis of quality. The term assessment is adopted in this article, consistent with the majority of methodologies, which stress the importance of the causes of poor data quality.
\end{comment}

The assessment and improvement phases of the DQ methodologies are organized in the framework of a common set of basic steps. The steps of the assessment phase are:
\begin{itemize}
    \item Data Analysis, which examines data schemas and performs interviews to reach a complete understanding of data and related architectural and management rules;
    \item DQ Requirements Analysis, which surveys the opinion of data users and administrators to identify quality issues and set new quality targets;
    \item Identification of Critical Areas, which selects the most relevant databases and data flows to be assessed quantitatively;
    \item Process Modeling, which provides a model of the processes producing or updating data;
    \item Measurement of Quality, which selects the quality dimensions affected by the quality issues identified in the DQ requirements analysis step and defines corresponding metrics.
\end{itemize}
Metadata plays a relevant role in all stages of the assessment phase. Metadata often provide the information necessary to understand data and/or evaluate them.

The steps of the improvement phase includes:
\begin{itemize}
    \item Evaluation of Costs, which estimates the direct and indirect costs of data quality;
    \item Assignment of Process responsibilities, which identifies the process owners and defines their responsibilities on data production and management activities;
    \item Assignment of Data Responsibilities, which identifies the data owners and defines their data management responsibilities;
    \item Identification of the Causes of Errors, which identifies the causes of quality problems;
    \item Selection of Strategies and Techniques, which identifies all the data improvement strategies and corresponding techniques, that comply with contextual knowledge, quality objectives, and budget constraints;
\end{itemize}
    Other common steps are Design of Data Improvement Solutions, Process Control,  Process Redesign, Improvement Management, Improvement Monitoring.
\begin{comment}
    \item Design of Data Improvement Solutions, which selects the most effective and efficient strategy and related set of techniques and tools to improve data quality;
    \item  Process Control, which defines check points in the data production processes, to monitor quality during process execution;
    \item Process Redesign, which defines the process improvement actions that can deliver corresponding DQ improvements;
    \item Improvement Management, which defines new organizational rules for data quality;
    \item Improvement Monitoring, which establishes periodic monitoring activities that provide feedback on the results of the improvement process and enables its dynamic tuning.
\end{comment}

\subsection{Strategies and Techniques}
There are two types of strategies: data-driven and process-driven. Data-driven strategies improve the quality of data by directly modifying the value of data and Process-driven strategies improve quality by redesigning the processes that create or modify data.

The common Data-Driven Techniques includes:
\begin{itemsize}
    \item Acquisition of New Data, which improves data by acquiring higher-quality data to replace the values that raise quality problems.
    \item Standardization, which replaces or complements nonstandard data values with corresponding values that comply with the standard. For example, nicknames are replaced with corresponding names, for example, Bob with Robert, and abbreviations are replaced with corresponding full names, for example, Channel Str. with Channel Street.
    \item Record Linkage, which identifies that data representations in two (or multiple) tables that might refer to the same real-world object;
    \item Data and Schema Integration, which define a unified view of the data provided by heterogeneous data sources. Integration has the main purpose of allowing a user to access the data stored by heterogeneous data sources through a unified view of these data.
    \item Source Trustworthiness, which selects data sources on the basis of the quality of their data;
    \item Error Localization and Correction, which identify and eliminate data quality errors by detecting the records that do not satisfy a given set of quality rules. These techniques are mainly in the statistical domain.
    \item Cost optimization, defines quality improvement actions along a set of dimensions by minimizing costs.
\end{itemsize}

\citet{Borek2011AClassficationOfDataQualityAssessmentMethod} also summarized other data-driven techniques, such as Column analysis, Cross-domain analysis, Data validation, Domain analysis,Lexical analysis
,Matching algorithms: identify duplicates
,Primary key and foreign key analysis (PK/FK analysis) : are good candidates for a PK/FK
,Schema matching: two attributes are semantically equivalent, and Semantic profiling.

The common Process-Driven Techniques includes:
\begin{itemize}
    \item Process control inserts checks and control procedures in the data production process when: (1) new data are created, (2) data sets are updated, or (3) new data sets are accessed by the process. In this way, a reactive strategy is applied to data modification events, thus avoiding data degradation and error propagation.
    \item Process redesign redesigns processes in order to remove the causes of poor quality and introduces new activities that produce data of higher quality.
\end{itemize}

\subsection{Cost}
The cost of a data quality program can be considered a preventive cost that is incurred by organizations to reduce data errors. This cost category includes the cost of all phases and steps that compose a data quality assessment and improvement process. \cite{Batini2009MethodologiesForDataQuality}
The costs of poor quality can be classified as:
\begin{itemize}
    \item Process Costs, such as the costs associated with the re-execution of the whole process due to data errors;
    \item Opportunity Costs, which is due to lost and missed revenues.
\end{itemize}
 The cost of poor data quality is strongly context-dependent as opposed to the cost of a data quality program. This makes its evaluation particularly difficult, as the same data value and corresponding level of quality has a different impact depending on the recipient. For example, an active trader receiving obsolete information on a stock may incur considerable economic losses as a consequence of wrong investment decisions. In contrast, a newspaper receiving the same obsolete information to publish monthly trading reports may not experience any economic loss.

\subsection{Classification of Methodologies}
Methodologies can be classified into four categories based on their emphasis on improvement, assessment, cost and quality dimensions. \cite{Batini2009MethodologiesForDataQuality}  The categories are:
\begin{enumerate}
    \item Complete methodologies, which provide support to both the assessment and improvement phases, and address both technical and economic issues;
    \item Audit methodologies, which focus on the assessment phase and provide limited support to the improvement phase;
    \item Operational methodologies, which focus on the technical issues of both the assessment and improvement phases, but do not address economic issues.
    \item Economic methodologies, which focus on the evaluation of costs.
\end{enumerate}

Figures \ref{fig:classificationMethodologies} shows the methods positioned on a two-dimensional coordinated. \cite{Batini2009MethodologiesForDataQuality}. The acronyms, extended names and main references are shown in Table \ref{tab:Methodologies-Acronyms}.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Paper/figures/Method Overview A classification of methodologies.png}
    \caption{A classification of methodologies \cite{Batini2009MethodologiesForDataQuality}}
    \label{fig:classificationMethodologies}
  \end{figure}

\begin{table}[]
\begin{tabular}{|l|l|l|}
\hline
Methodology Acronym & Extended Name & Main Reference \\ \hline
TDQM & Total Data Quality Management & \citet{Wang1998TDQM} \\ \hline
DWQ & The Datawarehouse Quality Methodology & \citet{1999DQW} \\ \hline
TIQM & Total Information Quality Management & \citet{English1999TIQM} \\ \hline
AIMQ & A methodology for information quality assessment & \citet{2002AIMQ} \\ \hline
CIHI & Canadian Institute for Health Information methodology & \citet{2002CIHI} \\ \hline
DQA & Data Quality Assessment & \citet{Pipino2002DQA} \\ \hline
IQM & Information Quality Measurement & \citet{Eppler2002MeasuringIQ} \\ \hline
ISTAT & ISTAT methodology & \citet{2002ISTAT} \\ \hline
AMEQ & \begin{tabular}[c]{@{}l@{}}Activity-based Measuring and Evaluating \\ of product information Quality (AMEQ) methodology\end{tabular} & \citet{2006AMEQ} \\ \hline
COLDQ & Loshin Methodology (Cost-effect Of Low Data Quality) & \citet{2004COLDQ} \\ \hline
DaQuinCIS & Data Quality in Cooperative Information Systems & \citet{Scannapieco2002} \\ \hline
QAFD & Methodology for the Quality Assessment of Financial Data & \citet{2006QAFD} \\ \hline
CDQ & Comprehensive methodology for Data Quality management & \citet{Batini2008ComprehensiveDQ} \\ \hline
\end{tabular}
\caption{Methodologies Acronyms in Figure \ref{fig:classificationMethodologies} \cite{Batini2009MethodologiesForDataQuality}}
\label{tab:Methodologies-Acronyms}
\end{table}


\section{Case Study: Total Data Quality Management}\label{sec:TDQM}


This section introduces the TDQM methodology and its applications.  The reason TDQM is chosen as a case study is that TDQM is a complete and general-purpose methodology. It also suggests a complete set of relevant dimensions and improvement methods that can be applied in different contexts.
The TDQM methodology was the first general methodology published in the data quality literature. \cite{Wang1998TDQM}
TDQM's goal is to support the entire end-to-end quality improvement process, from requirements analysis to implementation.
TDQM is based on the principles of Total Quality Management (TQM). \cite{OAKLAND1999TQM}  In the original paper, the word Information is used instead of Data. For convenience, these two words will be used interchangeably. TDQM proposes a language for the description of \emph{Information Production} (IP) processes.  It identifies four roles for the IP process:
\begin{itemize}
    \item \emph{Information Suppliers}, which create or collect data for the IP,
    \item \emph{Information Manufacturers}, which design, develop, or maintain data and related system infrastructure,
    \item \emph{Information Consumers}, which use data in their work,
    \item \emph{Information Process Managers}, which are responsible for managing the entire information production process throughout the information life cycle.
The roles are responsible for the different phases of the quality improvement process.
\end{itemize}
\subsection{Data Types and Quality Dimensions}
TDQM supports both structured and semistructured data typed. It refers to DQ as Information Quality (IQ) and categorized them into four categories as shown in Table \ref{table:TDQMIQcatergories}.

\begin{table}[]
\centering
\begin{tabular}{|c|c|}
\hline
IQ Category         & IQ Dimensions                                                                                                                        \\ \hline
Intrinsic IQ        & Accuracy, Objectivity, Believability, Reputation                                                                                     \\ \hline
Accessibility IQ    & Access, Security                                                                                                                     \\ \hline
Contextual IQ       & \begin{tabular}[c]{@{}c@{}}Relevancy, Value-Added, Timeliness,\\ Completeness, Amount of data\end{tabular}                           \\ \hline
Representational IQ & \begin{tabular}[c]{@{}c@{}}Interpretability, Ease of understanding, Concise\\ representation, Consistent representation\end{tabular} \\ \hline
\end{tabular}
\caption{Information quality categories and dimensions \cite{Wang1998TDQM}}
\label{table:TDQMIQcatergories}
\end{table}

\subsection{Phases and Steps}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Paper/figures/TDQM-2.png}
    \caption{TDQM Phases \cite{Wang1998TDQM}}
    \label{fig:PhasesTDQM}
 \end{figure}

The TDQM has four cycles: Define, Measure, Analyze,
and Improve. A schematic of the TDQM methodology is shown in Figure \ref{fig:PhasesTDQM}. The tasks embedded in the methodology are performed iteratively to ensure high-quality IP. The Define, Measure, and Analyze cycle are considered as the assessment phase introduced in the section.  The last cycle is the improvement phase.

\begin{enumerate}
    \item Define: Data analysis, DQ requirements analysis, process modeling are performed in this phase. The input of this phase is IP. The output consists of three-part:
    \begin{itemize}
        \item The logical and physical design of the Information Product with the necessary quality attributes
        \item A quality entity-relationship model that defines the IP and its IQ requirements
        \item An information manufacturing system that describes how the IP has been produced
    \end{itemize}
    Define IP Characteristics is a data analysis task. It defines characteristics at two levels: (1) high level: description of functionalities for information consumers
(2) low-level description of the basic units and components of the information product and their relationships
Define IQ Requirements is a DQ requirements analysis. It defines the IQ requirements from the perspective of four roles of IP: suppliers, manufacturers, consumers, and managers.
Define Information Manufacturing System is a process modeling task.
TDQM uses an information production map, IP-MAP, a graphical model designed to help them to visualize the information production process.
    \item  Measure: The input of this phase is IQ dimensions from the definition phase. The output is IQ problems. The main tasks are Definition of the IQ metrics and measurement of IP
    \item Analyze: The Inputs are IQ problems. Outputs are actions for improving data quality. The main task is to Analyze IP. The purpose is to identify the causes of errors and discrepancies.
    \item Improve: Inputs are  IQ metrics. Outputs are IQ improvement techniques. The main task is to Improve IP. It identifies key areas for improvement and selects suitable strategies, and techniques.
\end{enumerate}
\subsection{Strategies and Techniques}
TDQM focus on process-driven strategies and techniques. TDQM provides guidelines to apply process-driven strategies by using the Information Manufacturing Analysis Matrix\cite{Ballou1998ModelingInformation}, which suggests when and how to improve data. The description of processes is a mandatory activity, consistent with the general orientation of process-driven strategies. After modeling and assessing the information production process, new process control activities are identified, and/or process redesign decisions are taken.  Complex solutions such as IP-MAP cannot always be adopted due to their high costs and, in some cases, the practical unfeasibility of a thorough process modeling step. For this reason, other methodologies adopt less formal, but more feasible solutions.

\subsection{Example}

In the paper, the business needs for private client services in an investment bank is used as an illustrative example. The schema of the company's client account database is shown in Figure \ref{fig:AccountSchema}.
In the Define phase, four roles are first identified. A broker who creates accounts and executes transactions has to collect from
clients the necessary information for opening accounts and executing these transactions.
The broker is a supplier. An information-systems professional who designs, develops, produces, or maintains the system is a manufacturer. A financial controller or a client representative who uses this system is a consumer. The manager who is responsible for the collection, manufacturing, and delivery of customer account data is an IP manager. After Define IP Characteristics, components of the database and their relationships are defined.
In the client account database, a client is identified by an
account number. Company stocks are identified by the
companies' stock ticker symbols. When a client makes
a trade (buy/sell), the date, quantity of shares, and trade
price is stored as a record of the transaction. An entity-relationship(ER) diagram is shown in Figure \ref{fig:Quality Entity-Relationship Diagram}. In Define IQ Requirements, a software tool is used for the survey to collect data from a manufacturer and a consumer.

From the IP characteristics and the IQ assessment
results, the corresponding logical and physical design of
the IP can be developed with the necessary quality
attributes incorporated [9]. Timeliness and credibility
are two important IQ dimensions (Dimensions 3 and 7)
for an IP supporting trading operations. In Figure \ref{fig:IQ added to the ER diagram},
timeliness on share price indicates the trader is concerned with how old the data is. These characteristics are depicted as a dotted-rectangle as shown in Figure \ref{fig:IQ added to the ER diagram}. For example, timeliness is redefined by age
(of the data), and the credibility of the research report is
redefined by analyst name.
After Define Information Manufacturing System, Figure \ref{fig:InformationManufacturingSystem} illustrates an
information manufacturing system that has five data units ($DU_{1}$ to $DU_{5}$) supplied by three vendors
($VB_{1}$ to $VB_{3}$). Three data units ($DU_{6}$, $DU_{8}$ to $DU_{10}$) are
formed by having been passed through one of the three
data quality blocks ($QB_{1}$ to $QB_{3}$). For example, $DU_{6}$ represents the impact of $QB_{1}$ on $DU_{2}$. The system has three consumers
($CB_{1}$ to $CB_{3}$). Each consumer receives some subset of the IP.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Paper/figures/TDQM A client account schema.png}
    \caption{A client account schema \cite{Wang1998TDQM}}
    \label{fig:AccountSchema}
 \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Paper/figures/TDQM A quality entity-relationship diagram.png}
    \caption{A Quality Entity-Relationship Diagram \cite{Wang1998TDQM}}
    \label{fig:Quality Entity-Relationship Diagram}
 \end{figure}


\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Paper/figures/TDQM IQ added to the ER diagram.png}
    \caption{IQ added to the ER diagram \cite{Wang1998TDQM}}
    \label{fig:IQ added to the ER diagram}
 \end{figure}


\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Paper/figures/TDQM_information_manufacturing_system.png}
    \caption{A Information Manufacturing System \cite{Wang1998TDQM}}
    \label{fig:InformationManufacturingSystem}
 \end{figure}

In Measure IP phase, IQ metrics may be designed to track, for example:
\begin{itemize}
    \item The percentage of incorrect client address zip codes
found in a randomly selected client account (free of
error)
    \item An indicator of when client account data were last
updated (timeliness or currency for database marketing and regulatory purposes)
    \item The percentage of non-existent accounts or the
number of accounts with missing value in the
industry-code field (completeness)
    \item The number of records that violate referentially
integrity (consistency)
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Paper/figures/TDQM The Information Manufacturing Analysis Matrix for the illustrative.png}
    \caption{The Information Manufacturing Analysis Matrix for the illustrative (partial view) \cite{Ballou1998ModelingInformation}}
    \label{fig:Information Manufacturing Analysis Matrix}
 \end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Paper/figures/TDQM Descriptive inputs required for the primitive data units.png}
    \caption{Descriptive inputs required for the primitive data units \cite{Ballou1998ModelingInformation}}
    \label{fig:Descriptive inputs required for the primitive data units}
 \end{figure}

In Improve Phase, the Information Manufacturing Analysis Matrix [3] is developed and shown in Figure /ref{fig:Information Manufacturing Analysis Matrix}. Figure \ref{fig:Information Manufacturing Analysis Matrix} presents the descriptive inputs required to compute the timeliness, quality, cost, and value characteristics of information products to be delivered to an array of customers. Figure \ref{fig:Information Manufacturing Analysis Matrix} identifies the seven descriptive inputs required for each of the five primary data units. For example, $DU_{2}$ is obtained from the first vendor at a cost of 10, is of intermediate quality, and is already 2-time units old when it enters the system at the beginning of the information manufacturing process. It is highly volatile with a shelf file of only 30-time units and a second-degree timeliness function. Based on the descriptive inputs from Figure /ref{fig:Information Manufacturing Analysis Matrix}, an Information Manufacturing Analysis Matrix for this system is developed and shown in Figure \ref{fig:Descriptive inputs required for the primitive data units}.
 IP team can use the matrix to identify key areas for improvement, such as aligning information flow and work flow with the corresponding information manufacturing system and realigning the key characteristics of the IP with business needs.

\subsection{Application}
TDQM has been applied to several areas. Early TDQM versions are reported in several U.S.A. Department of Defence (DoD) documents \cite{DoD1994}. the methodology has been the basis for a law enforcement tool \citep{Sessions2007EmployingTT}. Other applications of TDQM performed at the DoD Medical Command for Military Treatment Facilities (MTF) are reported in \citet{Wang1998TDQM} and \citet{COREY1996}. A claimed advantage of TDQM is that based on target payoffs, critical DQ issues, and the corresponding types of data, one can effectively evaluate how representative and comprehensive the DQ metrics are and whether this is the right set of metrics. This is the reason for its extensive application in different contexts, such as insurance companies, as described in \citet{Nadkarni2006} and  \citet{Patrick2005}.

Recent use of TDQM is found in \citet{Bowo2019CaseStudy}. PT JAS is a non-bank financial institution engaged in sharia guarantee. The company's main business is managing risk. By using TDQM, companies can find out in advance the condition of the quality of company data and immediately formulate strategies and steps to improve and develop the quality of the data they have, so that data can be a useful and valuable asset. The company focus on aspects of confidentiality, integrity, and availability. With TDQM, the defined dimensions and measure result are as following:
\begin{itemize}
    \item Completeness: 66 fields found with a percentage of 99\% data completeness and 5 fields with a percentage of from 27\% to 33\% meet the criteria
    \item Validity: 2 out of 9 fields of data did not meet the criteria with 0.59\% (v5) and 3.39\% (v6) error rate
    \item Accuracy: 4 out of 9 fields did not meet the criteria with  1.25\% (acc3), 3.08\% (acc5), 1.83\% (acc6), and 2.24\% (acc9)

\end{itemize}
TDQM provides a common framework for facilitating understanding in data improvement approach through data quality management .
Nowadays, there are also many contributions extending TDQM, by improving IP-MAP \cite{Scannapieco2002, Shankaranarayanan2007IPMAPC} or by proposing a TDQM based Capability Maturity Model \citep{Baskarada2006}.

\section{Conclusion}
This paper first introduces the fundamentals of data type, DQ problems, and dimensions.  Thus answers the research topic of data properties that can be analyzed and cannot be analyzed.

In the second section, the overview of DQ assessment methods is presented. The pipelines, phases, steps, strategy, techniques, cost, and classification of general assessment methods are presented. This part answers the research topic. Moreover, for the research topic of improving DQ, two improvement strategies, data-driven, and process-driven are presented along with corresponding techniques.
In the third section, a particular DQ assessment method, TDQM, is brought out for a case study, The case study gives a concrete picture of how a DQ assessment is designed, implemented, and executed. In subsections of the third section, an example on the investment bank and three real-world applications of TDQM on the existing data set are presented. This response to the research topic of application of the DQ assessment methods. With all research topics covered, a complete evaluation of the DQ assessment method is performed. The readers should obtain a fundamental understanding of DQ assessment methods.

Challenges exist for data assessment in the Big Data era. \cite{Katal2013BigData} The characteristics of big data come down to the 4Vs: Volume, Velocity, Variety, and Value. Volume refers to the tremendous volume of the data. Velocity means that data are being formed at an unprecedented speed and must be dealt with promptly.   The growth of the data sources in both size and scope consequently has significantly increased the complexity of data quality management.
Variety indicates that big data has all kinds of data types, and this diversity divides the data into structured data and unstructured data. These multitype data requires higher data processing capabilities.
Finally, Value represents low-value density. Value density is inversely proportional to total data size, the greater the big data scale, the less relatively valuable the data. Consequently, the cost of DQ assessment will gain significant importance is inevitable. How to accurately define and estimate the cost of DQ is another topic.

In the end, experience suggests there are no  “one size fits all”  DQ methodology. Assessing data quality is an on-going effort that requires awareness of the fundamental principles underlying the development and context of the application.

\bibliography{template}

\end{document}
